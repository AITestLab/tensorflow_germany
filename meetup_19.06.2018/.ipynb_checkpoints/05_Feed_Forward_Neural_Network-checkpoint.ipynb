{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buliding Feed Forward Neural Network\n",
    "\n",
    "Most probably, Neural Network is the first step in the deep learning.The name Deep Learning comes from the concept that computer scientists hope to mimic the brain structure with the same functionality of the neurons. The key feature of this model is that it can separate data which is NOT linearly separable. We assume that you have the basic knowledge over the concept and you are just interested in the Tensorflow implementation of the Neural Nets. If you want to know more about the Neural Nets we suggest you to take this amazing course on machine learning. To build any classifier, your code needs specific parts:\n",
    "\n",
    "Prepare the needed libraries, input data and hyper-parameters for the network\n",
    "    1.Build the graph of the network\n",
    "    2.Train the network\n",
    "\n",
    "#### Prepration:\n",
    "_Imports: We will start with importing the required libraries._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "print('Tensorflow Version : {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import tensorflow package for loading the data**\n",
    "\n",
    "Note: This module will be depreceated in future tensorflow version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('data', one_hot=True, fake_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(mnist.train.labels)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(mnist.test.labels)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(mnist.validation.labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Util  \n",
    "\n",
    "Below is plot images util function which we will use for plotting the true and predicted class labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None, title=None):\n",
    "    \"\"\"\n",
    "    Create figure with 3x3 sub-plots.\n",
    "    :param images: array of images to be plotted, (9, img_h*img_w)\n",
    "    :param cls_true: corresponding true labels (9,)\n",
    "    :param cls_pred: corresponding true labels (9,)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    img_h = img_w = np.sqrt(images.shape[-1]).astype(int)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape((img_h, img_w)), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            ax_title = \"True: {0}\".format(np.argmax(cls_true[i]))\n",
    "        else:\n",
    "            ax_title = \"True: {0}, Pred: {1}\".format(np.argmax(cls_true[i]), cls_pred[i])\n",
    "\n",
    "        ax.set_title(ax_title)\n",
    "\n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    if title:\n",
    "        plt.suptitle(title, size=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example_errors(images, cls_true, cls_pred, title=None):\n",
    "    \"\"\"\n",
    "    Function for plotting examples of images that have been mis-classified\n",
    "    :param images: array of all images, (#imgs, img_h*img_w)\n",
    "    :param cls_true: corresponding true labels, (#imgs,)\n",
    "    :param cls_pred: corresponding predicted labels, (#imgs,)\n",
    "    \"\"\"\n",
    "    # Negate the boolean array.\n",
    "    incorrect = np.logical_not(np.equal(cls_pred, np.argmax(cls_true,axis=1)))\n",
    "\n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    incorrect_images = images[incorrect]\n",
    "\n",
    "    # Get the true and predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "    cls_true = cls_true[incorrect]\n",
    "\n",
    "    # Plot the first 9 images.\n",
    "    plot_images(images=incorrect_images[0:9],\n",
    "                cls_true=cls_true[0:9],\n",
    "                cls_pred=cls_pred[0:9],\n",
    "                title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get images for plotting - here we are getting first 9 images and labels \n",
    "images = mnist.train.images[0:9]\n",
    "images_class_true = mnist.train.labels[0:9]\n",
    "plot_images(images, images_class_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters:\n",
    "Hyper-parameters are important parameters which are not learned by the network. So, we have to specify them externally. These parameters are constant and they are not learnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "learning_rate = 0.001  # The optimization learning rate\n",
    "epochs = 10  # Total number of training epochs\n",
    "batch_size = 128  # Training batch size\n",
    "display_freq = 100  # Frequency of displaying the training results\n",
    "\n",
    "# Network Parameters\n",
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_h = img_w = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_h * img_w\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "n_classes = 10\n",
    "\n",
    "# number of units in the first hidden layer\n",
    "hidden_units_first_layer = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, img_size_flat], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, n_classes], name='y-input')\n",
    "    keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Readings\n",
    "\n",
    "[Activation Functions](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
    ",[Searching for activation function](https://arxiv.org/pdf/1710.05941.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer.\n",
    "        It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "        It also sets up name scoping so that the resultant graph is easy to read, and\n",
    "        adds a number of summary ops.\n",
    "    \"\"\"\n",
    "    # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "    with tf.name_scope(layer_name):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = weight_variable([input_dim, output_dim])\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = bias_variable([output_dim])\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "        activations = act(preactivate, name='activation')\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Network\n",
    "#The networki will be with one hidden layer with 500 units (change if you want just to see the difference)\n",
    "hidden1 = nn_layer(x, img_size_flat, hidden_units_first_layer, 'layer1')\n",
    "#Added dropout layer \n",
    "dropped = tf.nn.dropout(hidden1, keep_prob)\n",
    "#Output Layer with softmax activation function whichwill give propbabilities about the predicted class\n",
    "y = nn_layer(dropped, hidden_units_first_layer, n_classes, 'layer2', act=tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('prediction'):\n",
    "    class_prediction = tf.argmax(y, axis=1, name='predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('cross_entropy'):\n",
    "    diff = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y)\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Readings\n",
    "\n",
    "[Matrix Calculus](http://explained.ai/matrix-calculus/index.html), [Overvview of Gradient Descent](http://ruder.io/optimizing-gradient-descent/), [Neural networks: training with backpropagation](https://www.jeremyjordan.me/neural-networks-training/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('adam_optimizer'):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter('/notebooks/graphs' + '/dnn', sess.graph)\n",
    "    # Number of training iterations in each epoch\n",
    "    num_tr_iter = int(mnist.train.num_examples // batch_size)\n",
    "    print('Total Number of itteration : {}'.format(num_tr_iter))\n",
    "    for epoch in range(epochs):\n",
    "        print('Training epoch: {}'.format(epoch+1))\n",
    "        for iteration in range(num_tr_iter):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Run optimization op (backprop)\n",
    "            feed_dict_batch = {x: batch_x, y_: batch_y, keep_prob:0.3}\n",
    "            _ = sess.run(train_step, feed_dict=feed_dict_batch)\n",
    "\n",
    "            if iteration % display_freq == 0:\n",
    "            # Calculate and display the batch loss and accuracy\n",
    "                loss_batch, acc_batch = sess.run([loss, accuracy],\n",
    "                                                 feed_dict=feed_dict_batch)\n",
    "                print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".format(iteration, \n",
    "                                                                                        loss_batch, \n",
    "                                                                                        acc_batch))\n",
    "        \n",
    "        # Run validation after every epoch\n",
    "        feed_dict_valid = {x: mnist.validation.images, y_: mnist.validation.labels, keep_prob: 1.0}\n",
    "        loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "        print('---------------------------------------------------------')\n",
    "        print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".format(epoch + 1, \n",
    "                                                                                           loss_valid, \n",
    "                                                                                           acc_valid))\n",
    "        print('---------------------------------------------------------')\n",
    "    # Test the network after training\n",
    "    # Accuracy\n",
    "    feed_dict_test = {x: mnist.test.images, y_: mnist.test.labels, keep_prob:1.0}\n",
    "    cls_pred,loss_test, acc_test = sess.run([class_prediction,loss, accuracy], feed_dict=feed_dict_test)\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"Test loss: {0:.2f}, test accuracy: {1:.01%}\".format(loss_test, acc_test))\n",
    "    print('---------------------------------------------------------')\n",
    "\n",
    "    # Plot some of the correct and misclassified examples\n",
    "    cls_true = mnist.test.labels\n",
    "    plot_images(mnist.test.images, cls_true, cls_pred, title='Examples')\n",
    "    plot_example_errors(mnist.test.images, cls_true, cls_pred, title='Misclassified Examples')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen some of the examples are even hard for human to be classified. But we will see in the future that there are networks (like Convolutional Neural Networks) that can reduce the classification error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reccomended Book \n",
    "[Deep Learning](http://www.deeplearningbook.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
